---
title: 'Homework 3'
author: 'Forhad Akbar'
date: '04/09/2021'
output:
   rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



\newpage


```{r, message=FALSE,warning=FALSE, echo=F}
# loading libraries
library(tidyverse)
library(caret)
library(pROC)
library(knitr)
library(Amelia)
library(naniar)
library(reshape2)
library(stats)
library(corrplot)
library(e1071)
library(jtools)
library(performance)
library(rJava)
library(glmulti)
library(cvms)
library(ROCR)
```


# Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not(0). <br>
<br>
Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set: <br>

- zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
- indus: proportion of non-retail business acres per suburb (predictor variable)
- chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
- nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)
- rm: average number of rooms per dwelling (predictor variable)
- age: proportion of owner-occupied units built prior to 1940 (predictor variable)
- dis: weighted mean of distances to five Boston employment centers (predictor variable)
- rad: index of accessibility to radial highways (predictor variable)
- tax: full-value property-tax rate per $10,000 (predictor variable)
- ptratio: pupil-teacher ratio by town (predictor variable)
- black: 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town (predictor variable)
- lstat: lower status of the population (percent) (predictor variable)
- medv: median value of owner-occupied homes in $1000s (predictor variable)
- target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)   

# Data Exploration

```{r}
training <- read.csv('./crime-training-data_modified.csv')
training2 <- training # for melting and box plot
evaluation <- read.csv('./crime-evaluation-data_modified.csv')

training %>% head() %>% kable() 

# Converting to factor
var <- c("chas","target")
training[,var] <- lapply(training[,var], as.factor)
evaluation$chas <- as.factor(evaluation$chas)

```
Checking missing data

```{r, message=FALSE, warning=FALSE}
missmap(training, main="Missing Values") # using Amelia package
colSums(is.na(training))
```

```{r, message=FALSE, warning=FALSE}
# Boxplot to see distributions with target variable
melt(training2, id.vars='target') %>% mutate(target = as.factor(target)) %>% 
  ggplot(., aes(x=variable, y=value))+geom_boxplot(aes(fill=target))+facet_wrap(~variable, dir='h',scales='free')+ labs(title="BoxPlot - Predictors Data Distribution with Target Variable")

# Correlation matrix among variables
training2 %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", tl.cex=.8, diag = FALSE)

# Correlation table 
correlation <- training2 %>% 
  cor(., use = "complete.obs") %>%
  as.data.frame() %>%
  rownames_to_column()%>%
  gather(Variable, Correlation, -rowname) 

correlation %>%
  filter(Variable == "target") %>%
     arrange(desc(Correlation)) %>%
  kable() 

# Density plot to check normality
melt(training2, id.vars='target') %>% mutate(target = as.factor(target)) %>% 
  ggplot(., aes(x=value))+geom_density(fill='gray')+facet_wrap(~variable, scales='free')+
  labs(title="Density Plot for Normality and Skewness") + 
  theme_classic()

# Skewness and outliers
sapply(training2, skewness, function(x) skewness(x))
```

# Data Preparation

## Data Splitting

```{r, message=FALSE, warning=FALSE}
# Data splitting into train and test datasets out of training2
set.seed(1003)
training_partition <- createDataPartition(training2$target, p=0.7, list = FALSE, times=1)
train2 <- training2[training_partition, ]
test2 <- training2[-training_partition, ]

sapply(training2, skewness, function(x) skewness(x))
```

## log transformation

```{r, warning=FALSE, message=FALSE}
train_log <- train2 # copy of basic model for log transformation
test_log <- test2


train_log$zn <- log10(train_log$zn + 1)
test_log$zn <- log10(test_log$zn + 1)

# Plot and check skewness
sapply(train_log, skewness, function(x) skewness(x))
ggplot(melt(train_log), aes(x=value))+geom_density()+facet_wrap(~variable, scales='free') + labs(title="Log Transformation")

```

## BoxCox Transformation

```{r, message=FALSE, warning=FALSE}
# Copy of train and test
train_boxcox <- train2
test_boxcox <- test2

# Preprocessing
preproc_value <- preProcess(train2[,-1] , c("BoxCox", "center", "scale"))

# Transformation on both train and test datasets
train_boxcox_transformed <- predict(preproc_value, train_boxcox)
test_boxcox_transformed <- predict(preproc_value, test_boxcox)

ggplot(melt(train_boxcox_transformed), aes(x=value))+geom_density()+facet_wrap(~variable, scales='free') + labs(title="BoxCox Transformation")
sapply(train_boxcox_transformed, function(x) skewness(x))
```


# Build Models

## Model 1 - Backward elimination on Log Transformed data


```{r, message=FALSE,warning=FALSE}
# creating model1
model1 <- lm(target ~ nox + age+ rad+ medv, family= binomial, data= train_log)
summ(model1)
check_collinearity(model1) %>% kable(caption="Multicollinearity")
```

## Model 2 - Backward elimination on BoxCox Transformed data


```{r, message=FALSE, warning=FALSE}
model2 <- lm(target ~ nox + age + dis + rad +  medv, family= binomial, data= train_boxcox_transformed)
summ(model2)
check_collinearity(model2) %>% kable(caption="Multicollinearity")
```



## Model 3 - Using Stepwise Regression


```{r, message=FALSE,warning=FALSE}
model3 <- step(model2)
summ(model3)
```

## Model 4 - Using glmulti


```{r, message=FALSE, warning=FALSE}
# Model4 using glmulti()
model4 <- glmulti(target ~ ., data = train2, level = 1, method="h", crit = "aic", plotty = FALSE, fitfunction = "glm", family=binomial)
```

```{r, message=FALSE, warning=FALSE}
summary(model4@objects[[1]]) 
```

# Select Models

```{r, message=FALSE,warning=FALSE}
compare_performance(model1, model2, model3, rank = TRUE) %>% kable()
```

Model 4

```{r}
model_performance(model4@objects[[1]]) %>% kable() 
```


## Prediction Accuracy



```{r, warning=FALSE, message=FALSE}
test3 <- test2 # copy of test dataset 
test3$target <- as.factor(test3$target)

# Calculating confusion matrix for model1
preds1 <- predict(model1, newdata = test3)
preds1[preds1 > 0.5] = 1
preds1[preds1 < 0.5] = 0
preds1 <- as.factor(preds1)
model1_cm <- confusionMatrix(preds1, test3$target,mode="everything")
tidy1 <- tidy(model1_cm[[2]])
model1_cm
# plot_confusion_matrix
plot_confusion_matrix(tidy1, target_col = "Prediction", prediction_col = "Reference",counts_col = "n", add_zero_shading = FALSE) + ggplot2::labs(title = "Confusion Matrix Model 1")


# Calculating confusion matrix for model2
preds2 <- predict(model2, newdata = test3)
preds2[preds2 > 0.5] = 1
preds2[preds2 < 0.5] = 0
preds2 <- as.factor(preds2)
model2_cm <- confusionMatrix(preds2, test3$target,mode="everything")
tidy2 <- tidy(model2_cm[[2]])
model2_cm
# plot_confusion_matrix
plot_confusion_matrix(tidy2, target_col = "Prediction", prediction_col = "Reference",counts_col = "n", add_zero_shading = FALSE) + ggplot2::labs(title = "Confusion Matrix Model 2")

# Calculating confusion matrix for model3
preds3 <- predict(model3, newdata = test3)
preds3[preds3 > 0.5] = 1
preds3[preds3 < 0.5] = 0
preds3 <- as.factor(preds3)
model3_cm <- confusionMatrix(preds3, test3$target,mode="everything")
tidy3 <- tidy(model3_cm[[2]])
model3_cm
# plot_confusion_matrix
plot_confusion_matrix(tidy3, target_col = "Prediction", prediction_col = "Reference", counts_col = "n", add_zero_shading = FALSE) + ggplot2::labs(title = "Confusion Matrix Model 3")

# Calculating confusion matrix for model4
preds4 <- predict(model4@objects[[1]], newdata = test3)
preds4[preds4 > 0.5] = 1
preds4[preds4 < 0.5] = 0
preds4 <- as.factor(preds4)
model4_cm <- confusionMatrix(preds4, test3$target,mode="everything")
tidy4 <- tidy(model4_cm[[2]])
model4_cm
# plot_confusion_matrix
plot_confusion_matrix(tidy4, target_col = "Prediction", prediction_col = "Reference", counts_col = "n", add_zero_shading = FALSE) + ggplot2::labs(title = "Confusion Matrix Model 4")

```

Model 4 performed best

## Predicting the test set


```{r, message=FALSE,warning=FALSE}
evaluation$target <- round(predict(model4@objects[[1]], evaluation),3)
evaluation <- evaluation %>% mutate(target = if_else(evaluation$target < 0.5, 0,1))
evaluation %>% head()
```

